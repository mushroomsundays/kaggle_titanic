from wsgiref.validate import validator
import numpy as np
import pandas as pd 
from matplotlib import pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score # split data
from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer # for scoring during model selection
from sklearn.model_selection import GridSearchCV, RepeatedKFold # cross validation
from sklearn.metrics import confusion_matrix # creates a confusion matrix
from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix
from sklearn.metrics import mean_squared_error as MSE
from sklearn.feature_selection import SelectFromModel, RFE, RFECV
from sklearn.metrics import accuracy_score

def main():
    # Read in cleaned data generated by explore.py
    X = pd.read_csv('data/X.csv')
    y = pd.read_csv('data/y.csv')

    print(X.head())
    print(X.columns)
    print(y.head())
    print(y.columns)

    # Split into train and test
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

    # Build the model, a forest of extreme gradient boosted trees
    clf_xgb = xgb.XGBClassifier(objective='binary:logistic', missing=1, seed=42)
    print("Fitting model...")
    # TODO: early_stopping_rounds depricated
    # plot_confusion_matrix depricated
    clf_xgb.fit(X_train,
        y_train,
        verbose=True,
        early_stopping_rounds=10,
        eval_metric='aucpr',
        eval_set=[(X_test, y_test)])
    """
    # Plot confusion matrix
    print("Plotting confusion matrix...")
    plot_confusion_matrix(clf_xgb,
        X_test,
        y_test,
        values_format='d',
        display_labels=["Did not survive", "Survived"])
    # TODO: am I sure about the labels?
    print("Saving confusion matrix...")
    plt.savefig('confusion_matrix.png')
    print("Confusion matrix saved.")
    """

    xgb.plot_importance(clf_xgb)
    plt.savefig('features1.png')

    # Automated recursive feature elimination
    scores = {}
    for n in range(1,17):
        print("-----------------------------------------------")
        print(f"Building RFE for {n} features...")
        rfe = RFE(estimator=clf_xgb, n_features_to_select=n)
        rfe.fit(X_train, y_train)
        # Print features in order of ranking
        from operator import itemgetter
        features = X_train.columns.to_list()
        for x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):
            print(x, y)
        score = rfe.score(X_test, y_test)
        scores.update({n: score})
    for k,v in scores.items():
        print(f"Score for {k} parameters: {v}")
        """
        Score for 1 parameters: 0.7668161434977578
        Score for 2 parameters: 0.7668161434977578
        Score for 3 parameters: 0.7488789237668162
        Score for 4 parameters: 0.7488789237668162
        Score for 5 parameters: 0.7668161434977578
        Score for 6 parameters: 0.7802690582959642
        Score for 7 parameters: 0.7713004484304933
        Score for 8 parameters: 0.7757847533632287
        Score for 9 parameters: 0.7757847533632287
        Score for 10 parameters: 0.7668161434977578
        Score for 11 parameters: 0.7847533632286996
        Score for 12 parameters: 0.7802690582959642
        Score for 13 parameters: 0.7802690582959642
        Score for 14 parameters: 0.7802690582959642
        Score for 15 parameters: 0.7802690582959642
        Score for 16 parameters: 0.7802690582959642
        """
        # 11 parameters is best, but 6 is very close
        # do we choose 6 to prevent overfitting? idk
        # I think we want to cross validate this as well
    
    # why is the plot_importance rank way different than this one?

    # Recursive feature elimination with cross validation
    # Defaults: step=1, min_features_to_select=1, cv=5-fold, n_jobs=1, importance_getter=coef_ or feature_importances_
    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html
    print("Building feature elimination with RFECV...")
    rfecv = RFECV(estimator=clf_xgb)
    rfecv.fit(X_train, y_train)
    scorecv = rfecv.score(X_test, y_test)
    print(f"Score: {scorecv}") # 0.780269
    print(f"Optimal number of features selected with cross-validation: {rfecv.n_features_}") # 12
    for feature_name, ranking in zip(rfecv.feature_names_in_, rfecv.ranking_):
        print(f"{feature_name}: {ranking}")
    # Dropped:
    #  Embarked_Q
    #  CabinLetter_B
    #  CabinLetter_F
    #  CabinLetter_G

    # Plot number of features VS. cross-validation scores
    min_features_to_select = 1
    plt.figure()
    plt.xlabel("Number of features selected")
    plt.ylabel("Cross validation score (accuracy)")
    plt.plot(
        range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),
        rfecv.grid_scores_,
    )
    plt.savefig('rfecv_scores.png')

    # View correlation of remaining features
    X_corr = X.corr()
    X_corr.to_csv('correlation.png', index=False)

    # Principle component analysis

    """
    # Feature selection
    # Fit model using each importance as a threshold
    print(X.dtypes)
    thresholds = np.sort(clf_xgb.feature_importances_)
    for thresh in thresholds:
        # select features using threshold
        selection = SelectFromModel(clf_xgb, threshold=thresh, prefit=True)
        select_X_train = selection.transform(X_train)
        # train model
        selection_model = xgb.XGBClassifier()
        selection_model.fit(select_X_train, y_train)
        # eval model
        select_X_test = selection.transform(X_test)
        y_pred = selection_model.predict(select_X_test)
        predictions = [round(value) for value in y_pred]
        accuracy = accuracy_score(y_test, predictions)
        print("Thresh=%.3f, n=%d, Accuracy: %.2f%%" % (thresh, select_X_train.shape[1], accuracy*100.0))
    """
    """
    # Cross validation
    # Round 1
    print("Cross validation round 1...")
    params = {
        'max_depth': [3,4,5],
        'learning_rate': [0.1, 0.05, 0.01], # also known as 'eta'
        'gamma': [0, 0.25, 1],
        'reg_lambda': [0, 1, 10],
        'scale_pos_weight': [1,3,5] # xgboost recommends sum(negatives) / sum(positives) 1.6
    }
    grid = GridSearchCV(
        estimator = clf_xgb,
        param_grid=params,
        scoring='roc_auc',
        verbose=True,
        n_jobs=10,
        cv=5
    )
    grid.fit(X_train, y_train)

    #print(grid.cv_results_)
    print("Best estimator:")
    print(grid.best_estimator_)
    print("Best score:")
    print(grid.best_score_)
    print("Best params:")
    print(grid.best_params_)
    

    # Best params:
    # {'gamma': 0.25, 'learning_rate': 0.05, 'max_depth': 4, 'reg_lambda': 10, 'scale_pos_weight': 1}
    # adjusting reg_lambda up
    # seeing if scale_pos_weight can be a float

    # Round 2
    print("Cross validation round 2...")

    params = {
        'max_depth': [3,4,5],
        'learning_rate': [0.1, 0.05, 0.01], # also known as 'eta'
        'gamma': [0, 0.25, 1],
        'reg_lambda': [10, 20, 100],
        'scale_pos_weight': [1,1.5, 2] # xgboost recommends sum(negatives) / sum(positives) 1.6
    }
    grid = GridSearchCV(
        estimator = clf_xgb,
        param_grid=params,
        scoring='roc_auc',
        verbose=True,
        n_jobs=10,
        cv=5
    )
    grid.fit(X_train, y_train)

    #print(grid.cv_results_)
    print("Best estimator:")
    print(grid.best_estimator_)
    print("Best score:")
    print(grid.best_score_)
    print("Best params:")
    print(grid.best_params_)
    

    params = {
        'max_depth': [5],
        'learning_rate': [0.05], # also known as 'eta'
        'gamma': [0.25],
        'reg_lambda': [20],
        'scale_pos_weight': [1] # xgboost recommends sum(negatives) / sum(positives) 1.6
    }
    grid = GridSearchCV(
        estimator = clf_xgb,
        param_grid=params,
        scoring='roc_auc',
        verbose=True,
        n_jobs=10,
        cv=5
    )
    grid.fit(X_train, y_train)

    #print(grid.cv_results_)
    print("Best estimator:")
    print(grid.best_estimator_)
    print("Best score:")
    print(grid.best_score_)
    print("Best params:")
    print(grid.best_params_)

    # Build optimal model
    print("Building optimal model...")
    clf_xgb = xgb.XGBClassifier(
        seed=42,
        objective='binary:logistic',
        gamma=0.25,
        learning_rate=0.05,
        max_depth=5,
        reg_lambda=20,
        scale_pos_weight=1,
        subsample=0.9,
        colsample_bytree=1
    )

    clf_xgb.fit(X_train,
        y_train,
        verbose=True,
        early_stopping_rounds=10,
        eval_metric='aucpr',
        eval_set=[(X_test, y_test)])

    # Plot confusion matrix
    print("Plotting confusion matrix...")
    plot_confusion_matrix(clf_xgb,
        X_test,
        y_test,
        values_format='d',
        display_labels=["Did not survive", "Survived"])
    # TODO: am I sure about the labels?
    print("Saving confusion matrix...")
    plt.savefig('confusion_matrix_2.png')
    print("Confusion matrix saved.")

    print("Plotting feature importance...")
    xgb.plot_importance(clf_xgb)
    plt.savefig('feature_importance.png')

    # Load test csv
    df_test = pd.read_csv('data/test.csv')

    # Create submission csv
    # Change sex to IsFemale (bool) (less memory)
    replace_values = {'female': 1, 'male': 0}
    df_test['IsFemale'] = df_test['Sex'].copy().replace(replace_values).astype(bool)
    df_test['CabinLetter'] = [x[0] if not isinstance(x, float) else x for x in df_test['Cabin']]
    X_new = df_test.drop(['Name', 'Sex', 'Ticket', 'PassengerId', 'Cabin'], axis=1).copy()
    X_new_encoded = pd.get_dummies(X_new, columns=['Embarked', 'CabinLetter'])

    print("X_encoded columns:")
    print(X_encoded.dtypes)
    print("X_new_encoded columns:")
    print(X_new_encoded.dtypes)

    yhat = clf_xgb.predict(X_new_encoded)
    print(df_test['PassengerId'])
    print(len(df_test['PassengerId']))
    print(yhat)
    print(len(yhat))
    yhat_series = pd.Series(yhat, name='Survived')

    submission = df_test['PassengerId'].to_frame().join(yhat_series)[['PassengerId', 'Survived']]
    submission.to_csv('data/results.csv', index=False)

    full_result = pd.concat([X_new_encoded, submission], axis=1)
    full_result.to_csv('data/full_result.csv', index=False)

    # After filling 'Age' with median, score dropped from 0.777 to 0.65???
    """
if __name__ == "__main__":
    main()