import numpy as np
import pandas as pd 
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix # creates a confusion matrix
from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix
from sklearn.metrics import mean_squared_error as MSE
from sklearn.feature_selection import SelectFromModel, RFE, RFECV
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score
from sklearn.metrics import precision_recall_curve, roc_curve, auc, log_loss
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.preprocessing import StandardScaler

def main():
    # Read in cleaned data generated by explore.py
    X = pd.read_csv('data/X.csv')
    y = pd.read_csv('data/y.csv')

    print(X.head())
    print(X.columns)
    print(y.head())
    print(y.columns)
    ########################################
    # Split into train and test
    ########################################
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

    ########################################
    # Fit Logistic Regression model 
    ########################################
    # .values gives the values in a numpy array (shape: (n,1))
    # .ravel() will convert that array to shape (n, ) (i.e. flatten it)
    clf = LogisticRegression(max_iter=500, random_state=42).fit(X_train, y_train.values.ravel())

    ########################################
    # Recursive feature elimination
    ########################################
    print("Fitting RFECV...")
    rfecv = RFECV(estimator=clf, step=1, cv=10, scoring='accuracy').fit(X_train, y_train.values.ravel())
    print("Optimal number of features: %d" % rfecv.n_features_)
    for feature_name, ranking in zip(rfecv.feature_names_in_, rfecv.ranking_):
        print(f"{feature_name}: {ranking}")
    
    score = clf.score(X_test, y_test)
    scorecv = rfecv.score(X_test, y_test)

    print(f"Score for logistic regression with all features: {score}")
    print(f"Score for rfecv: {scorecv}")

    # Plot number of features VS. cross-validation scores
    """
    min_features_to_select = 1
    plt.figure()
    plt.xlabel("Number of features selected")
    plt.ylabel("Cross validation score (accuracy)")
    plt.plot(
        range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),
        rfecv.grid_scores_,
    )
    plt.savefig('rfecv_scores.png')
    """

    # Drop 'Fare', the only feature that wasn't selected
    X_train.drop(['Fare'], axis=1, inplace=True)
    X_test.drop(['Fare'], axis=1, inplace=True)
    """
    # Plot correlation
    plt.subplots(figsize=(14, 5))
    sns.heatmap(X_train.corr(), annot=True, cmap="RdYlGn")
    plt.savefig('corr.png')
    """

    ########################################
    # Rebuild model based on RFECV (without Fare)
    ########################################
    clf = LogisticRegression(max_iter=500, random_state=42).fit(X_train, y_train.values.ravel())
    y_pred = clf.predict(X_test)
    y_pred_proba = clf.predict_proba(X_test)[:, 1] # what is this?
    [fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)
    accuracy = accuracy_score(y_test, y_pred)
    logloss = log_loss(y_test, y_pred_proba)
    area_under_curve = auc(fpr, tpr)
    print(f"Accuracy score: {accuracy}")
    print(f"Log loss: {logloss}")
    print(f"Area under curve: {area_under_curve}")

    ########################################
    # Model evaluation based on K-fold cross validation
    ########################################
    # We pass X and y instead of X_train and Y_train because cross validation
    # takes care of splitting the data
    scores_accuracy = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='accuracy')
    scores_log_loss = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='neg_log_loss')
    scores_auc = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='roc_auc')
    print("----------K-fold cross validation results----------")
    print(f"Accuracy: {scores_accuracy.mean()}")
    print(f"Log loss: {-1*scores_log_loss.mean()}")
    print(f"AUC: {scores_auc.mean()}")

    ########################################
    # GridSearchCV evaluating multiple scorers simultaneously (hyperparameter tuning)
    # Note: Feature engineering can drastically improve model performance,
    # while hyperparameter tuning usually results in only small improvement
    ########################################
    print("----------GridSearchCV results----------")
    param_grid = {'C': np.arange(1e-05, 3, 0.1)}
    scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}
    gs = GridSearchCV(clf, return_train_score=True,
                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')
    gs.fit(X, y.values.ravel())
    results = gs.cv_results_
    print('='*20)
    print("best params: " + str(gs.best_estimator_))
    print("best params: " + str(gs.best_params_))
    print('best score:', gs.best_score_)
    print('='*20)



    ########################################
    # Use pipeline to define the steps your data goes through in model building
    ########################################
    


if __name__ == "__main__":
    main()
