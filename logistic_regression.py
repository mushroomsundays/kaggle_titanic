import numpy as np
import pandas as pd 
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix # creates a confusion matrix
from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix
from sklearn.metrics import mean_squared_error as MSE
from sklearn.feature_selection import SelectFromModel, RFE, RFECV
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score
from sklearn.metrics import precision_recall_curve, roc_curve, auc, log_loss
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.preprocessing import StandardScaler

def k_fold(X, y, clf):
    # We pass X and y instead of X_train and Y_train because cross validation
    # takes care of splitting the data
    scores_accuracy = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='accuracy')
    scores_log_loss = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='neg_log_loss')
    scores_auc = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='roc_auc')
    print("----------K-fold cross validation results----------")
    print(f"Accuracy: {scores_accuracy.mean()}")
    print(f"Log loss: {-1*scores_log_loss.mean()}")
    print(f"AUC: {scores_auc.mean()}")

def gridsearch_cv(X, y, clf):
    print("----------GridSearchCV results----------")
    param_grid = {'C': np.arange(1e-05, 3, 0.1)}
    scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}
    gs = GridSearchCV(clf, return_train_score=True,
                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')
    gs.fit(X, y.values.ravel())
    results = gs.cv_results_
    print('='*20)
    print("best params: " + str(gs.best_estimator_))
    print("best params: " + str(gs.best_params_))
    print('best score:', gs.best_score_)
    print('='*20)

def prep_test_data(df):
    # Fill Embarked with the most likely value (S)
    df['Embarked'].fillna('S', inplace=True)

    # Age is continuous; fill nulls with median
    df['Age'] = df['Age'].fillna(df['Age'].dropna().median())
    # After plotting age vs. survival, we see that children under 16 survived far more often
    # Create a column IsChild
    df['IsChild'] = np.where(df['Age'] <= 16, 1, 0).astype(bool)

    # Explore Cabin Column
    print(df['Cabin'].unique())
    # Cabin: C85, B28, etc. There are also NaN
    # Pull letter out of Cabin and treat as categorical (get dummy)
    # This deals with missing values by turning into 0 in dummy column
    df['CabinLetter'] = [x[0] if not isinstance(x, float) else x for x in df['Cabin']]
    print(df['CabinLetter'].unique())
    print(df['CabinLetter'].value_counts())
    """
    C    59
    B    47
    D    33
    E    32
    A    15
    F    13
    G     4
    T     1
    """
    # Fill null cabin with C, the most commonly occurring
    df['CabinLetter'] = df['CabinLetter'].fillna('C')

    # SibSp and Parch both relate to traveling with family, so combine them
    # to account for multicollinearity and for simplicity
    df['IsAlone']=np.where((df["SibSp"]+df["Parch"])>0, 0, 1)

    # Change sex to IsFemale (bool) (less memory)
    replace_values = {'female': 1, 'male': 0}
    df['IsFemale'] = df['Sex'].copy().replace(replace_values).astype(bool)

    #print(f"How many females survived? {len(df[(df['Survived'] == 1) & (df['IsFemale'] == 1)])}")
    #print(f"How many females died? {len(df[(df['Survived'] == 0) & (df['IsFemale'] == 1)])}")
    #print(f"How many males survived? {len(df[(df['Survived'] == 1) & (df['IsFemale'] == 0)])}")
    #print(f"How many males died? {len(df[(df['Survived'] == 0) & (df['IsFemale'] == 0)])}")

    # Drop unused columns

    X = df.drop(['Name', 'Sex', 'Ticket', 'PassengerId', 'Cabin', 'SibSp', 'Parch'], axis=1).copy()

    # One hot encoding
    X_encoded = pd.get_dummies(X, columns=['Embarked', 'CabinLetter', 'Pclass'])
    
    # We can always drop one categorical column; if the others are 0 then we know the person
    # falls into the last category
    X_encoded.drop(['Embarked_Q', 'CabinLetter_G', 'Pclass_3', 'Fare'], axis=1, inplace=True)

    return X_encoded

def main():
    # Read in cleaned data generated by explore.py
    X = pd.read_csv('data/X.csv')
    y = pd.read_csv('data/y.csv')

    print(X.head())
    print(X.columns)
    print(y.head())
    print(y.columns)
    ########################################
    # Split into train and test
    ########################################
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

    ########################################
    # Fit Logistic Regression model 
    ########################################
    # .values gives the values in a numpy array (shape: (n,1))
    # .ravel() will convert that array to shape (n, ) (i.e. flatten it)
    clf = LogisticRegression(max_iter=500, random_state=42).fit(X_train, y_train.values.ravel())

    ########################################
    # Recursive feature elimination
    ########################################
    print("Fitting RFECV...")
    rfecv = RFECV(estimator=clf, step=1, cv=10, scoring='accuracy').fit(X_train, y_train.values.ravel())
    print("Optimal number of features: %d" % rfecv.n_features_)
    for feature_name, ranking in zip(rfecv.feature_names_in_, rfecv.ranking_):
        print(f"{feature_name}: {ranking}")
    
    score = clf.score(X_test, y_test)
    scorecv = rfecv.score(X_test, y_test)

    print(f"Score for logistic regression with all features: {score}")
    print(f"Score for rfecv: {scorecv}")

    # Plot number of features VS. cross-validation scores
    """
    min_features_to_select = 1
    plt.figure()
    plt.xlabel("Number of features selected")
    plt.ylabel("Cross validation score (accuracy)")
    plt.plot(
        range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),
        rfecv.grid_scores_,
    )
    plt.savefig('rfecv_scores.png')
    """

    # Drop 'Fare', the only feature that wasn't selected
    X_train.drop(['Fare'], axis=1, inplace=True)
    X_test.drop(['Fare'], axis=1, inplace=True)
    """
    # Plot correlation
    plt.subplots(figsize=(14, 5))
    sns.heatmap(X_train.corr(), annot=True, cmap="RdYlGn")
    plt.savefig('corr.png')
    """

    ########################################
    # Rebuild model based on RFECV (without Fare)
    ########################################
    clf = LogisticRegression(max_iter=500, random_state=42).fit(X_train, y_train.values.ravel())
    y_pred = clf.predict(X_test)
    y_pred_proba = clf.predict_proba(X_test)[:, 1] # what is this?
    [fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)
    accuracy = accuracy_score(y_test, y_pred)
    logloss = log_loss(y_test, y_pred_proba)
    area_under_curve = auc(fpr, tpr)
    print(f"Accuracy score: {accuracy}")
    print(f"Log loss: {logloss}")
    print(f"Area under curve: {area_under_curve}")

    ########################################
    # Model evaluation based on K-fold cross validation
    ########################################
    #k_fold(X, y, clf)

    ########################################
    # GridSearchCV evaluating multiple scorers simultaneously (hyperparameter tuning)
    # Note: Feature engineering can drastically improve model performance,
    # while hyperparameter tuning usually results in only small improvement
    ########################################
    #gridsearch_cv(X, y, clf)

    ########################################
    # Predict and output results
    ########################################
    df = pd.read_csv('data/test.csv')
    passenger_ids = df['PassengerId']
    X_test = prep_test_data(df)
    print(X_test.isna().any())

    yhat = clf.predict(X_test)
    yhat_series = pd.Series(yhat, name='Survived').replace({True: 1, False: 0})
    submission = passenger_ids.to_frame().join(yhat_series)[['PassengerId', 'Survived']]
    submission.to_csv('data/results_lr.csv', index=False)



    ########################################
    # Use pipeline to define the steps your data goes through in model building
    ########################################
    


if __name__ == "__main__":
    main()
